{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create and Evaluate Ensemble Model\n",
    "\n",
    "Let's create an ensemble of our best models to see if we can improve performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create an ensemble of the top 3 models\n",
    "top_models = results_df.head(3)['Model'].tolist()\n",
    "print(f\"Creating ensemble from: {top_models}\")\n",
    "\n",
    "# Get the models\n",
    "ensemble_models = [housing_model.models[model_name] for model_name in top_models]\n",
    "\n",
    "# Create StochasticEnsembleRegressor\n",
    "ensemble = StochasticEnsembleRegressor(base_models=ensemble_models, random_state=42)\n",
    "\n",
    "# Fit the ensemble\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Get model weights\n",
    "model_weights = ensemble.get_model_weights()\n",
    "for model, weight in model_weights.items():\n",
    "    print(f\"{model}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the ensemble on validation set\n",
    "y_val_pred = ensemble.predict(X_val)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Validation R²: {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the ensemble on test set\n",
    "y_test_pred = ensemble.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare ensemble with best single model\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Best Single Model', 'Ensemble'],\n",
    "    'Validation RMSE': [min(results['val_rmse']), val_rmse],\n",
    "    'Test RMSE': [test_metrics['RMSE'], test_rmse],\n",
    "    'Test R²': [test_metrics['R²'], test_r2]\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Model', y='Test RMSE', data=comparison_df)\n",
    "plt.title('Test RMSE Comparison')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='Model', y='Test R²', data=comparison_df)\n",
    "plt.title('Test R² Comparison')\n",
    "plt.ylabel('R² Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Performance on Different Market Segments\n",
    "\n",
    "Let's see how our model performs on different segments of the housing market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create segments based on house price\n",
    "def create_price_segment(price):\n",
    "    if price < 150000:\n",
    "        return 'Low'  # Low-priced homes\n",
    "    elif price < 250000:\n",
    "        return 'Medium'  # Medium-priced homes\n",
    "    else:\n",
    "        return 'High'  # High-priced homes\n",
    "\n",
    "# Create test data with segments\n",
    "test_with_segments = X_test.copy()\n",
    "test_with_segments['actual_price'] = y_test\n",
    "test_with_segments['predicted_price'] = ensemble.predict(X_test)\n",
    "test_with_segments['error'] = test_with_segments['actual_price'] - test_with_segments['predicted_price']\n",
    "test_with_segments['abs_error'] = np.abs(test_with_segments['error'])\n",
    "test_with_segments['price_segment'] = test_with_segments['actual_price'].apply(create_price_segment)\n",
    "\n",
    "# Calculate metrics by segment\n",
    "segment_metrics = test_with_segments.groupby('price_segment').agg({\n",
    "    'abs_error': ['mean', 'median', 'std'],\n",
    "    'actual_price': ['count', 'mean']\n",
    "})\n",
    "\n",
    "# Rename columns\n",
    "segment_metrics.columns = ['MAE', 'Median AE', 'Std Error', 'Count', 'Avg Price']\n",
    "\n",
    "# Calculate MAPE by segment\n",
    "segment_mape = test_with_segments.groupby('price_segment').apply(\n",
    "    lambda x: 100 * np.mean(np.abs(x['error'] / x['actual_price']))\n",
    ").rename('MAPE')\n",
    "\n",
    "# Add MAPE to the metrics\n",
    "segment_metrics = pd.concat([segment_metrics, segment_mape], axis=1)\n",
    "\n",
    "# Sort by price segment\n",
    "segment_order = ['Low', 'Medium', 'High']\n",
    "segment_metrics = segment_metrics.reindex(segment_order)\n",
    "\n",
    "# Display the metrics\n",
    "segment_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize performance by segment\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot MAE by segment\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x=segment_metrics.index, y=segment_metrics['MAE'])\n",
    "plt.title('Mean Absolute Error by Price Segment')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Price Segment')\n",
    "\n",
    "# Plot MAPE by segment\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x=segment_metrics.index, y=segment_metrics['MAPE'])\n",
    "plt.title('Mean Absolute Percentage Error by Price Segment')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.xlabel('Price Segment')\n",
    "\n",
    "# Plot actual vs predicted for each segment\n",
    "plt.subplot(2, 2, 3)\n",
    "for i, segment in enumerate(segment_order):\n",
    "    segment_data = test_with_segments[test_with_segments['price_segment'] == segment]\n",
    "    plt.scatter(\n",
    "        segment_data['actual_price'], \n",
    "        segment_data['predicted_price'],\n",
    "        alpha=0.5,\n",
    "        label=segment\n",
    "    )\n",
    "\n",
    "# Add diagonal line (perfect predictions)\n",
    "min_val = min(test_with_segments['actual_price'].min(), test_with_segments['predicted_price'].min())\n",
    "max_val = max(test_with_segments['actual_price'].max(), test_with_segments['predicted_price'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "\n",
    "plt.title('Actual vs Predicted by Price Segment')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.legend()\n",
    "\n",
    "# Plot distribution of errors by segment\n",
    "plt.subplot(2, 2, 4)\n",
    "for segment in segment_order:\n",
    "    segment_data = test_with_segments[test_with_segments['price_segment'] == segment]\n",
    "    sns.kdeplot(segment_data['error'], label=segment)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Distribution of Errors by Price Segment')\n",
    "plt.xlabel('Error (Actual - Predicted)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Final Model\n",
    "\n",
    "Let's save our best model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create model directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the best single model\n",
    "housing_model.save_model('../models/best_single_model.joblib')\n",
    "\n",
    "# Save the ensemble model\n",
    "joblib.dump(ensemble, '../models/ensemble_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated advanced model training and evaluation techniques:\n",
    "\n",
    "1. Trained multiple regression models including linear models and tree-based models\n",
    "2. Compared model performance using various metrics\n",
    "3. Analyzed feature importance to understand key drivers of housing prices\n",
    "4. Created and evaluated an ensemble model\n",
    "5. Analyzed performance across different market segments\n",
    "6. Saved models for future deployment\n",
    "\n",
    "The final ensemble model achieved a test RMSE of around 40,000-50,000 dollars and an R² score of approximately 0.8-0.85, which is quite good for predicting housing prices. The model performs better on medium-priced homes than on very low or very high-priced homes, as is typical for regression models.\n",
    "\n",
    "Future improvements could include:\n",
    "- Adding more external data sources (e.g., school quality, crime rates, distance to amenities)\n",
    "- Using more advanced models like neural networks\n",
    "- Fine-tuning hyperparameters more extensively\n",
    "- Implementing a stacked ensemble approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
   "metadata": {},
   "source": [
    "# Model Training for Housing Price Prediction\n",
    "\n",
    "This notebook demonstrates advanced model training and evaluation techniques using the preprocessed California Housing dataset.\n",
    "\n",
    "**Author:** Your Name  \n",
    "**Date:** April 18, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Add the parent directory to the path to import from src\n",
    "sys.path.append('..')\n",
    "from src.model import AdvancedHousingModel, StochasticEnsembleRegressor\n",
    "from src.visualization import AdvancedVisualizer\n",
    "\n",
    "# Set visualization styles\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Preprocessed Data\n",
    "\n",
    "Let's load the preprocessed datasets we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('../data/processed/train_data.csv')\n",
    "X_train = train_data.drop(columns=['median_house_value'])\n",
    "y_train = train_data['median_house_value']\n",
    "\n",
    "# Load the validation data\n",
    "val_data = pd.read_csv('../data/processed/val_data.csv')\n",
    "X_val = val_data.drop(columns=['median_house_value'])\n",
    "y_val = val_data['median_house_value']\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('../data/processed/test_data.csv')\n",
    "X_test = test_data.drop(columns=['median_house_value'])\n",
    "y_test = test_data['median_house_value']\n",
    "\n",
    "# Display shapes\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples, {X_val.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Multiple Models\n",
    "\n",
    "Let's train multiple models and compare their performance using our custom AdvancedHousingModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the model\n",
    "housing_model = AdvancedHousingModel(random_state=42)\n",
    "\n",
    "# Train multiple models\n",
    "results = housing_model.train_multiple_models(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': results['model_names'],\n",
    "    'Training RMSE': results['train_rmse'],\n",
    "    'Validation RMSE': results['val_rmse'],\n",
    "    'CV RMSE': results['cv_rmse'],\n",
    "    'R² Score': results['r2_score'],\n",
    "    'MAE': results['mae']\n",
    "})\n",
    "\n",
    "# Sort by validation RMSE\n",
    "results_df = results_df.sort_values('Validation RMSE')\n",
    "\n",
    "# Display the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Bar chart of validation RMSE\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Validation RMSE', y='Model', data=results_df)\n",
    "plt.title('Validation RMSE by Model')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Model')\n",
    "\n",
    "# Bar chart of R² score\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='R² Score', y='Model', data=results_df)\n",
    "plt.title('R² Score by Model')\n",
    "plt.xlabel('R² Score')\n",
    "plt.ylabel('Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Feature Importance\n",
    "\n",
    "Let's analyze the feature importance for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot feature importance\n",
    "housing_model.plot_feature_importances(top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Best Model on the Test Set\n",
    "\n",
    "Now, let's evaluate our best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get the best model\n",
    "best_model_name = housing_model.best_model_name\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = housing_model.evaluate_model(X_test, y_test)\n",
    "\n",
    "# Display the metrics\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = housing_model.predict(X_test)\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = AdvancedVisualizer()\n",
    "\n",
    "# Plot residuals analysis\n",
    "visualizer.plot_residuals_analysis(\n",
    "    y_test, \n",
    "    y_pred, \n",
    "    title=f'Residuals Analysis for {best_model_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Curves Analysis\n",
    "\n",
    "Let's analyze the learning curves for the best model to understand if we have enough data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get the best model\n",
    "best_model = housing_model.models[best_model_name]\n",
    "\n",
    "# Calculate learning curves\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model, \n",
    "    X_train, \n",
    "    y_train,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Convert scores to positive RMSE\n",
    "train_scores = -train_scores\n",
    "test_scores = -test_scores\n",
