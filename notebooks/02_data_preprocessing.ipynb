{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Housing Price Prediction\n",
    "\n",
    "This notebook demonstrates advanced data preprocessing techniques using Pandas and NumPy for the California Housing dataset.\n",
    "\n",
    "**Author:** Your Name  \n",
    "**Date:** April 18, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path to import from src\n",
    "sys.path.append('..')\n",
    "from src.data_processor import DataProcessor\n",
    "from src.feature_engineering import AdvancedFeatureEngineer\n",
    "from src.visualization import AdvancedVisualizer\n",
    "\n",
    "# Set visualization styles\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset\n",
    "\n",
    "Let's load the California Housing dataset that we prepared in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the DataProcessor\n",
    "data_processor = DataProcessor(random_state=42)\n",
    "\n",
    "# Load the dataset\n",
    "housing_df = data_processor.load_data('../data/housing.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detect Feature Types and Analyze Missing Values\n",
    "\n",
    "Now we'll use our DataProcessor to identify numerical and categorical features, and check for any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detect feature types\n",
    "numerical_features, categorical_features = data_processor.detect_feature_types(housing_df)\n",
    "\n",
    "# Analyze missing values\n",
    "missing_info = data_processor.analyze_missing_values(housing_df)\n",
    "missing_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Outliers\n",
    "\n",
    "Next, we'll identify and handle outliers in the dataset using our DataProcessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detect outliers using boxplots\n",
    "visualizer = AdvancedVisualizer()\n",
    "visualizer.plot_boxplots(housing_df, features=numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle outliers using winsorization (capping at z-score of 3)\n",
    "housing_df_clean = data_processor.handle_outliers(\n",
    "    housing_df, \n",
    "    method='winsorize', \n",
    "    threshold=3.0\n",
    ")\n",
    "\n",
    "# Visualize the effect of outlier handling\n",
    "for feature in ['AveOccup', 'Population', 'AveRooms']:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=housing_df[feature])\n",
    "    plt.title(f'Before Outlier Handling: {feature}')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=housing_df_clean[feature])\n",
    "    plt.title(f'After Outlier Handling: {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Now we'll apply advanced feature engineering techniques to create new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstrate our custom feature creation using DataProcessor\n",
    "\n",
    "# Create interaction features\n",
    "housing_df_with_interactions = data_processor.create_interaction_features(housing_df_clean)\n",
    "\n",
    "# Create polynomial features\n",
    "housing_df_with_poly = data_processor.create_polynomial_features(\n",
    "    housing_df_with_interactions,\n",
    "    features=['MedInc', 'AveRooms', 'HouseAge'],\n",
    "    degree=2\n",
    ")\n",
    "\n",
    "# Create group statistics\n",
    "# Binning latitude and longitude first to create groups\n",
    "housing_df_with_poly['lat_bin'] = pd.qcut(housing_df_with_poly['Latitude'], q=10, labels=False)\n",
    "housing_df_with_poly['lon_bin'] = pd.qcut(housing_df_with_poly['Longitude'], q=10, labels=False)\n",
    "housing_df_with_poly['geo_bin'] = housing_df_with_poly['lat_bin'].astype(str) + '_' + housing_df_with_poly['lon_bin'].astype(str)\n",
    "\n",
    "# Create group statistics based on geographic bins\n",
    "housing_df_engineered = data_processor.create_group_statistics(\n",
    "    housing_df_with_poly,\n",
    "    group_col='geo_bin',\n",
    "    agg_cols=['MedInc', 'HouseAge', 'AveRooms'],\n",
    "    statistics=['mean', 'median']\n",
    ")\n",
    "\n",
    "# Drop the temporary binning columns\n",
    "housing_df_engineered = housing_df_engineered.drop(columns=['lat_bin', 'lon_bin'])\n",
    "\n",
    "# Display the shape of the engineered dataset\n",
    "print(f\"Original dataset shape: {housing_df.shape}\")\n",
    "print(f\"Engineered dataset shape: {housing_df_engineered.shape}\")\n",
    "\n",
    "# Display some of the new features\n",
    "new_features = [col for col in housing_df_engineered.columns if col not in housing_df.columns]\n",
    "print(f\"Number of new features created: {len(new_features)}\")\n",
    "print(\"Sample of new features:\")\n",
    "print(new_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Now use our AdvancedFeatureEngineer class for more feature engineering\n",
    "feature_engineer = AdvancedFeatureEngineer(\n",
    "    polynomial_degree=2,\n",
    "    interaction_features=True,\n",
    "    binning_features=True,\n",
    "    cyclical_features=True,\n",
    "    outlier_features=True,\n",
    "    transformation_features=True\n",
    ")\n",
    "\n",
    "# Prepare X and y\n",
    "X = housing_df_clean.drop(columns=['median_house_value'])\n",
    "y = housing_df_clean['median_house_value']\n",
    "\n",
    "# Fit and transform\n",
    "feature_engineer.fit(X)\n",
    "X_transformed = feature_engineer.transform(X)\n",
    "\n",
    "# Display the shape of the transformed dataset\n",
    "print(f\"Original X shape: {X.shape}\")\n",
    "print(f\"Transformed X shape: {X_transformed.shape}\")\n",
    "\n",
    "# Display some of the new features\n",
    "new_features = [col for col in X_transformed.columns if col not in X.columns]\n",
    "print(f\"Number of new features created: {len(new_features)}\")\n",
    "print(\"Sample of new features:\")\n",
    "print(new_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "\n",
    "With so many engineered features, we need to select the most important ones to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate feature importance using f_regression\n",
    "feature_importance = data_processor.efficient_feature_importance(\n",
    "    X_transformed, \n",
    "    y,\n",
    "    method='f_regression'\n",
    ")\n",
    "\n",
    "# Display top 20 features\n",
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize feature importance\n",
    "visualizer.plot_feature_importance(\n",
    "    feature_importance['Feature'].values,\n",
    "    feature_importance['F-Value'].values,\n",
    "    top_n=20,\n",
    "    title='Top 20 Features by F-Value'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate feature importance using mutual information\n",
    "feature_importance_mi = data_processor.efficient_feature_importance(\n",
    "    X_transformed, \n",
    "    y,\n",
    "    method='mutual_info'\n",
    ")\n",
    "\n",
    "# Display top 20 features\n",
    "feature_importance_mi.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize mutual information feature importance\n",
    "visualizer.plot_feature_importance(\n",
    "    feature_importance_mi['Feature'].values,\n",
    "    feature_importance_mi['Mutual Information'].values,\n",
    "    top_n=20,\n",
    "    title='Top 20 Features by Mutual Information'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection and Dimensionality Reduction\n",
    "\n",
    "Let's select important features based on our analysis and apply dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select top 30 features based on F-Value\n",
    "top_features = feature_importance['Feature'].head(30).tolist()\n",
    "\n",
    "# Create a new DataFrame with selected features\n",
    "X_selected = X_transformed[top_features]\n",
    "\n",
    "print(f\"Original shape: {X_transformed.shape}\")\n",
    "print(f\"Selected features shape: {X_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply dimensionality reduction with PCA\n",
    "visualizer.plot_dimensionality_reduction(\n",
    "    pd.concat([X_selected, pd.DataFrame({'median_house_value': y})], axis=1),\n",
    "    target='median_house_value',\n",
    "    method='pca',\n",
    "    n_components=3,\n",
    "    figsize=(14, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply dimensionality reduction with t-SNE for visualization\n",
    "visualizer.plot_dimensionality_reduction(\n",
    "    pd.concat([X_selected, pd.DataFrame({'median_house_value': y})], axis=1),\n",
    "    target='median_house_value',\n",
    "    method='tsne',\n",
    "    n_components=2,\n",
    "    figsize=(14, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preprocessing Pipeline Creation\n",
    "\n",
    "Now we'll create a preprocessing pipeline for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create the preprocessor pipeline\n",
    "preprocessor = data_processor.create_preprocessor(\n",
    "    impute_strategy='median',\n",
    "    categorical_strategy='onehot',\n",
    "    handle_outliers=True,\n",
    "    knn_impute=False\n",
    ")\n",
    "\n",
    "# Display the preprocessing pipeline\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train-Validation-Test Split\n",
    "\n",
    "Finally, let's split the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = data_processor.create_train_val_test_split(\n",
    "    housing_df_clean, \n",
    "    target_col='median_house_value',\n",
    "    test_size=0.2,\n",
    "    val_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for model training\n",
    "# Select top features for all sets\n",
    "X_train_selected = X_train[top_features]\n",
    "X_val_selected = X_val[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "# Save preprocessed datasets for the next notebook\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save the training data\n",
    "train_data = pd.concat([X_train_selected, pd.DataFrame({'median_house_value': y_train})], axis=1)\n",
    "train_data.to_csv('../data/processed/train_data.csv', index=False)\n",
    "\n",
    "# Save the validation data\n",
    "val_data = pd.concat([X_val_selected, pd.DataFrame({'median_house_value': y_val})], axis=1)\n",
    "val_data.to_csv('../data/processed/val_data.csv', index=False)\n",
    "\n",
    "# Save the test data\n",
    "test_data = pd.concat([X_test_selected, pd.DataFrame({'median_house_value': y_test})], axis=1)\n",
    "test_data.to_csv('../data/processed/test_data.csv', index=False)\n",
    "\n",
    "# Save the list of selected features\n",
    "with open('../data/processed/selected_features.txt', 'w') as f:\n",
    "    for feature in top_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(\"Preprocessed datasets saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "In this notebook, we've demonstrated advanced data preprocessing techniques using Pandas and NumPy:\n",
    "\n",
    "1. Loaded and analyzed the dataset\n",
    "2. Detected and handled outliers using multiple methods\n",
    "3. Created engineered features using various techniques:\n",
    "   - Interaction features\n",
    "   - Polynomial features\n",
    "   - Group statistics\n",
    "   - Transformations (log, sqrt, etc.)\n",
    "   - Categorical encoding\n",
    "4. Applied feature selection techniques\n",
    "5. Created preprocessing pipeline\n",
    "6. Split the data into training, validation, and test sets\n",
    "\n",
    "The preprocessed data is now ready for model training in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
